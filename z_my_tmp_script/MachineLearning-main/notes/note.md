### 1. sigmoid 函数

```python
    # 1. sigmoid函数也叫Logistic函数
# 2.  
```

#### Sigmoid函数表达式:

![img](https://bkimg.cdn.bcebos.com/formula/7e627f6301407c3d610d2c2cab711f3f.svg)

#### Sigmoid函数的图形如S曲线:

![img](https://bkimg.cdn.bcebos.com/pic/c9fcc3cec3fdfc03f23fbf16d73f8794a5c226dc?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2U4MA==,g_7,xp_5,yp_5/format,f_auto)

### 2. 似然函数

### 3. 梯度下降

### 4. 逻辑回归对阈值结果的影响

### 5. 过采样

    - smote 算法 
        -  欧式距离 , k近邻
        -  
    - imblearn 库 有SMOTE 过采样

### 6. 解决样本不均衡的方案

    1.1 下采样 
        1.1.1 把不规则的数据(多列), 数据数量,趋于均衡.
        1.1.2 两列不一样数量的数据,使用数据生成, 使得两列数据 达到一样的数据量级别,一样少. 
        1.1.3 sklean 中的 StandardScaler标准化模块 ,可以对数据进行 变换
        1.1.4 可能出现的问题
            - 数据量级减少,导致模型模拟不精确
    1.2 过采样
    
    1.3 交叉验证 (对训练数据验证,目的是求稳)
        - 将测试数据 分为3份.
            1 + 2 -->3 验证
            1 + 3 -->2 验证
            2 + 3 -->1 验证
        - 将 三种 验证结果的 值,进行平均. 作为 训练模型的参数
        - 使用 cross_validation import train_test_split 对 原始数据 进行切分操作
       
        - tp , fn  可以求得 recall召回结果 , 公式: recall = tp / (tp  + fn)  
             - tp : true positives(正类判定为正类) , 在混淆矩阵中位置 为 (1,1)
             - fp : false positives(负类判定为正类) ,
             - fn : false negatives (正类判定为负类)  在混淆矩阵中位置 (0,1)
             - tn :  true negatives(负类判定为负类) ,
        
    1.4 惩罚规则
        - l1   |w|
        - l2   loss + 1/2 * w**2 
    
    1.5 混淆矩阵
    
    1.6 模型评估方法
        - 精确度 , 即 在混淆矩阵中   (0,1) 和 (1,1) 结果和 / 全部样本数量和
        - recall 召回值,  检测结果/样本数量,例如 90/100  ,recall结果为 90%

### 小节

    - 1. 读取数据 ,对数据分布进行分析
    - 2. 对 某些列 差异较大时, 进行标准化 (变形,使得数据和训练的数据差异不大)
    - 3. 采用 过采样/下采样对数据 进行填充/移除(解决样本不均衡处理)
    - 4. 使用交叉验证,对训练后的模型进行评估
    - 5. 在使用模型对测试数据进行训练

### 7. 决策树

    - 决策树:从根节点 开始  一步步走到叶子节点(决策)
    - 所有数据最终都会落在叶子节点上,既可以做分类,也可以做回归
    - 树的结构:
        - 根节点: 第一个选择点
        - 非叶子节点与分支: 中间过程
        - 叶子节点: 最终的决策结果
    
    - 问题:
        id3:即id作为决策数据的根节点,虽信息增益会极大,但是作为决策数据的根节点不适合
        - 采用 
            -  信息增益率, 考虑自身熵值
            -  CART (GINI系数,gini系数同熵值,gini系数越小 信息增益越大)

### 8. 熵

    - 熵: 不确定性大,得到的熵值越大
        - 熵 越小, 概率越大
        - 熵 越大, 概率越小

    - 熵值趋于0/1时, 随机变量完全没有不确定性
    - 熵值 = 0.5时, 随机变量的不确定性最大

### 9. 信息增益

    - 熵值的增益, 
        例如 样本原熵值为10
            1. 某特征增益后的熵值 为9,信息增益为 1
            2. 某特征增益后的熵值 为5,信息增益为 5
            
            - 信息增益为5的,适合作为决策树的根节点

### 10. 决策树剪枝

    - 为什么剪枝: 决策树过拟合风险大,数据可以完全分到叶子节点上. (资源的浪费)   
    - 剪枝策略:
        - 预剪枝 : 边建立决策树 边进行剪枝
                - 可以限制深度,叶子节点数,叶子节点样本数,信息增益量等

        - 后剪枝 : 当建立完决策树后.在进行剪枝操作    
                - 通过 一定的衡量标准

### 11. 决策树的连续树 处理

    - 连续值取哪个分界点
        - 二分,将连续值进行离散化. 找到最中间的数值

### 12. 决策树模型参数:

    - criterion gini or entropy
    - splitter best or random 前者是在所有特征中最好的切分点,后者是在部分特征(数据量大的时候)
    - max_features None (所有),log2,sqrt,N特征小于50的时候一般使用所有的
    - max_depth 数据少 或 特征少的时候可以不管这个值, 如果模型样本量多,特征也多的情况下
    - min_samples_split     
        1. 如果 节点的样本数量少于 min_samples_split,则不会继续再尝试选择最优特征来划分,如果样本量不大,不需要管这个值.
        2. 如果 样本数量非常大,则推荐使用这个值
    - min_samples_leaf 
        1. 这个值限制了叶子节点的最少样本数,如果某叶子节点数目小于样本数. 则会和兄弟节点一起被剪枝
        2. 如果样本量不大,则不不需要管这个值,大些如10w条样本可以尝试5
    - min_weight_fraction_leaf 
        1. 这个限制了叶子节点所有样本的权重和最小值,如果各这个最小值,则会和兄弟节点一起被剪枝. 默认剪枝为0 ,就是不考虑权重问题
        2. 一般来说,如果有较多样本存在缺失值,或 分类树样本的分布类别偏差很大. 引入样本权重,需要注意这个值
    - max_leaf_nodes 
        1. 通过限制最大叶子节点数,可以防止过拟合. 默认是None,即不限制最大叶子节点数. 
        2. 如果加了限制,算法会建立在最大叶子节点数最优的决策树
        3. 如果特征不多,可以不考虑这个值.但是如果特惠装呢个较多,可以增加具体的值,可以通过交叉验证得到
    - class_weight 
        1. 指定样本各类别的权重,主要是为了防止训练集某些类别的样本过多导致训练的决策树过于偏向这些类别
        2. 自己指定个样本的权重,若使用balanced,则算法会自己计算权重,样本量少的类别所对应的样本权重会高
    - min_impurity_split 这个限制了决策树的增长,如果某节点的不纯度(基尼系数,信息增益,均方差,绝对差)小于这个阈值,则该节点不在生成子节点,即叶子节点
    - n_estimators : 要建立树的个数
    
### 13. 决策树模型的训练和得分
       - 使用 train_test_split 对数据进行切分,分为训练集和测试集
       - random_state 确保随机性唯一,即多次运行结果都一致

### 14. 决策树参数调节/随机森林参数调节
    - 使用 GridSearchCV 
    
```python3
tree_param_grid = {"min_samples_split": list((3, 6, 9)),
                   "n_estimators": list((10, 50, 100))}
# cv 对 训练集交叉验证
grid = GridSearchCV(RandomForestRegressor(), param_grid=tree_param_grid, cv=5)
```

### 15. 集成算法 Ensemble learning 
    - 目的 : 让机器学习效果更好
    - Bagging : 训练多个分类器 平均 
        - 并行训练一堆分类器
        - 典型 :随机森林
        - 随机 : 数据采样随机,特征选择随机  %60~%80之间的随机数据. 特征也可以随机
        - 森林 : 很多个决策树并行放在一起
    - Boosting : 从 弱 学习器开始加强.通过加权来训练
        - 典型: AdaBoost , Xgboost
        - Adaboost 会根据前一次的分类效果 调整数据权重
    - Stacking : 聚合多个分类或回归模型(分阶段来做)
        - 集合多个算法,再讲算法结果进行逻辑回归
### 16. 随机森林的优势
    - 能处理高维度(feature)的数据,并且不用选择特征
    - 训练完后,它能给出哪些feature比较重要
    - 容易做成并行化 方法,速度较快
    - 可以进行可视化展示,便于分析
### 小节
    ? 交叉验证KFold 存在问题 ----> 泰坦尼克号 测试 2022.2.13
    
### 17. 贝叶斯算法
    - 为了解决 逆概问题!
    - 正向概率
    - 逆向概率
        公式: 
![img](https://bkimg.cdn.bcebos.com/formula/541075978d219fa03aa9357894d4d8f4.svg)
    
    - 先验概率